{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da0446b-2399-4a67-9b21-7fef52ab6e6c",
   "metadata": {},
   "source": [
    "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfb423-3178-4a17-b4bf-139dcfc8a411",
   "metadata": {},
   "source": [
    "**Web scraping** is the process of extracting data from websites using software programs called web scrapers or crawlers. These programs crawl through the HTML code of a website and extract specific information that is relevant to the user's needs.\n",
    "\n",
    "Web scraping is used for various purposes, including:\n",
    "\n",
    "**Business Intelligence:** Companies use web scraping to gather data about their competitors, industry trends, and customer behavior. This data can then be used to make informed business decisions.\n",
    "\n",
    "**Research:** Researchers use web scraping to collect data from various sources, including social media platforms, news websites, and academic journals. This data can be used to conduct surveys, analyze trends, and make predictions.\n",
    "\n",
    "**E-commerce:** Online retailers use web scraping to gather pricing information from competitors' websites, which they can use to adjust their own prices and stay competitive in the market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92fbd0-c3db-4f94-8a18-ed626f296964",
   "metadata": {},
   "source": [
    "# Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c014a-d6a7-4af0-9bf1-48541c0bb8b0",
   "metadata": {},
   "source": [
    "There are several methods used for web scraping. Here are some of the most common:\n",
    "\n",
    "**Manual scraping:** This involves manually copying and pasting information from websites into a spreadsheet or other software. This is a time-consuming process, but it is still used in some cases where the amount of data to be scraped is small.\n",
    "\n",
    "**Parsing HTML:** This involves writing code to parse the HTML code of a website and extract specific information using libraries such as Beautiful Soup or lxml.\n",
    "\n",
    "**Web scraping software:** There are many web scraping software tools available that can extract data from websites automatically. Some examples include Octoparse, ParseHub, and Scrapy.\n",
    "\n",
    "**APIs:** Some websites offer APIs (Application Programming Interfaces) that allow developers to access their data in a structured format. This can be a more reliable and efficient way to extract data compared to web scraping.\n",
    "\n",
    "**Headless browsers:** Headless browsers, such as Puppeteer or Selenium, can be used to automate the process of visiting websites, interacting with them, and extracting data. This can be useful for scraping data from dynamic websites or websites that require user authentication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f7516-3e3d-4c24-9042-959eb3b9093e",
   "metadata": {},
   "source": [
    "# Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488cdd0c-84dd-4252-8a39-c53d1b9ee841",
   "metadata": {},
   "source": [
    "**Beautiful Soup** is a Python library that is used for web scraping purposes. It is designed for parsing HTML and XML documents and allows developers to easily extract data from web pages.\n",
    "\n",
    "It is widely used in data scraping and web crawling applications due to its ease of use and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a9f11-262a-4a51-9c07-dba7099b5a39",
   "metadata": {},
   "source": [
    "# Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20915b7d-edf3-44ad-966d-63f6d9c00910",
   "metadata": {},
   "source": [
    "Today's project utilized Flask to build a basic front-end web application. The project began by creating a Flask application that formed the foundation of the entire project.\n",
    "\n",
    "The web page featured a text box, allowing users to input a specific product name. Additionally, a button was included that could be clicked to request information about the product. Upon clicking the button, users were directed to a new page that displayed various product details such as pricing, ratings, and reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872639e-f684-4af0-a025-7168c2a7262d",
   "metadata": {},
   "source": [
    "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd527ff6-0709-4f74-8457-929f0cc3bc86",
   "metadata": {},
   "source": [
    "The AWS services used in the project are **Elastic Beanstalk** and **CodePipeline.**\n",
    "\n",
    "**Elastic Beanstalk** is a fully managed service provided by AWS that allows developers to quickly deploy, manage, and scale applications. It supports a variety of programming languages, including Python, which is commonly used for web scraping projects. Elastic Beanstalk can automatically handle tasks such as capacity provisioning, load balancing, and application health monitoring, which allows developers to focus on building their application rather than managing infrastructure.\n",
    "\n",
    "**CodePipeline** is a fully managed continuous delivery service provided by AWS that automates the software release process. It helps developers to build, test, and deploy their applications by automating the steps required to release new versions. CodePipeline supports a variety of sources, such as GitHub, AWS CodeCommit, and Amazon S3, and integrates with other AWS services such as Elastic Beanstalk to deploy applications automatically.\n",
    "\n",
    "In this project, **Elastic Beanstalk** was used to deploy the Flask application, while **CodePipeline** was used to automate the deployment process. This allowed the developers to easily deploy new versions of the application without having to manually configure and manage infrastructure. By leveraging these AWS services, the developers were able to build and deploy a scalable and reliable web scraping application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
